dataset_name_or_path: "path to data"
learning_rate: 0.0005
train_batch_size: 7680
eval_batch_size: 1024
gradient_accumulation_steps: 1
load_in_8bit: False
load_in_4bit: False
use_peft: False
gradient_checkpointing: True
random_seed: 44
peft_lora_r: 16
peft_lora_alpha: 32
logging_steps: 1
num_train_epochs: 96
warmup_steps: 10000
max_steps: -1
local_rank: -1
weight_decay: 0
description: Embedding
base_model_path: /temp/local-ssd/models/bert_small_cased
output_dir: /temp/local-ssd/data/sm/finetune/checkpoint/
deepspeed: deepspeed_config.json
lr_scheduler: cosine
temperature: 1
trainer: siglip-trainer
model_name: fkbert
emb_type: bos_token
query_max_len: 64
passage_max_len: 194
max_example_num_per_dataset: 10000000000
query_instruction_for_retrieval: null
passage_instruction_for_retrieval: null
train_group_size: 1
use_inbatch_neg: True
use_dataset_neg: False
